glove = GlobalVectors$new(word_vectors_size = 40, vocabulary = vocab, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01)
dim(wv_main)
wv_context = glove$components
dim(wv_context)
wv_main[1,1]
t(wv_context)[1,1]
word_vectors = wv_main + t(wv_context)
#建造“垃圾+廢物”向量
relation = word_vectors["廢物", , drop = FALSE] +
word_vectors["垃圾", , drop = FALSE]
#看和 “垃圾+廢物” 相關的10個詞
cos_sim = sim2(x = word_vectors, y = relation, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
# ----------------------------------------------------------------------------------------文字雲
z <- sort(table(cloud),decreasing = T)[1:50]
wordcloud2(z,
size = 2,
color = "random-light",
backgroundColor = "black",
shape = "triangle")
cutter <- worker(stop_word = "C:/Users/jeff6/Desktop/Github/alan/颱風專題/ptt/a.txt")
new_user_word(cutter, c("西半部","東半部","中南部","南部","北部","東部","西部","東北部","基隆",
"台北","新北","桃園","新竹","苗栗","台中","澎湖","金門","連江","馬祖",
"彰化","南投","雲林","嘉義","台南","高雄","屏東","台東","花蓮","宜蘭",
"外圍環流","能不能","明天",
"丹娜絲","山竹","瑪莉亞","泰利","谷超","天鴿","珊瑚","馬莎",
"海棠","尼莎","艾利","梅姬","馬勒卡","莫蘭蒂","尼伯特","杜鵑","天鵝",
"蘇迪勒","蓮花","昌鴻","紅霞","鳳凰","麥德姆","哈吉貝","菲特","天兔",
"康芮","潭美","西馬隆","蘇力","杰拉華","天秤","啟德","海葵","蘇拉",
"杜蘇芮","南瑪都","梅花","米雷","桑達","凡那比","南修","萊羅克",
"芭瑪","莫拉克","莫拉菲","薔蜜","哈格比","辛樂克","如麗","鳳凰",
"卡玫基","米塔","柯羅莎","韋帕","聖帕","梧提","帕布","珊珊","寶發",
"桑美","凱米","碧利斯","艾維尼","珍珠","龍王","丹瑞","卡努"))
cloud <- cutter[data]
#---------------------------------------------------------------------------------------------看關聯性
tokens=list(cloud)
class(tokens)
# 建構詞彙
it = itoken(tokens, progressbar = FALSE)
vocab = create_vocabulary(it)
vocab = prune_vocabulary(vocab, term_count_min = 5L)#刪除出現小於5次的詞
tail(vocab,20)#查看最高频的詞
vectorizer = vocab_vectorizer(vocab)
tcm = create_tcm(it, vectorizer,skip_grams_window = 5L)# 考慮詞的前後5個詞
glove = GlobalVectors$new(word_vectors_size = 40, vocabulary = vocab, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01)
dim(wv_main)
wv_context = glove$components
dim(wv_context)
wv_main[1,1]
t(wv_context)[1,1]
word_vectors = wv_main + t(wv_context)
#建造“垃圾+廢物”向量
relation = word_vectors["廢物", , drop = FALSE] +
word_vectors["垃圾", , drop = FALSE]
#看和 “垃圾+廢物” 相關的10個詞
cos_sim = sim2(x = word_vectors, y = relation, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
# ----------------------------------------------------------------------------------------文字雲
z <- sort(table(cloud),decreasing = T)[1:50]
wordcloud2(z,
size = 2,
color = "random-light",
backgroundColor = "black",
shape = "triangle")
cutter <- worker(stop_word = "C:/Users/jeff6/Desktop/Github/alan/颱風專題/ptt/a.txt")
new_user_word(cutter, c("西半部","東半部","中南部","南部","北部","東部","西部","東北部","基隆",
"台北","新北","桃園","新竹","苗栗","台中","澎湖","金門","連江","馬祖",
"彰化","南投","雲林","嘉義","台南","高雄","屏東","台東","花蓮","宜蘭",
"外圍環流","能不能","明天",
"丹娜絲","山竹","瑪莉亞","泰利","谷超","天鴿","珊瑚","馬莎",
"海棠","尼莎","艾利","梅姬","馬勒卡","莫蘭蒂","尼伯特","杜鵑","天鵝",
"蘇迪勒","蓮花","昌鴻","紅霞","鳳凰","麥德姆","哈吉貝","菲特","天兔",
"康芮","潭美","西馬隆","蘇力","杰拉華","天秤","啟德","海葵","蘇拉",
"杜蘇芮","南瑪都","梅花","米雷","桑達","凡那比","南修","萊羅克",
"芭瑪","莫拉克","莫拉菲","薔蜜","哈格比","辛樂克","如麗","鳳凰",
"卡玫基","米塔","柯羅莎","韋帕","聖帕","梧提","帕布","珊珊","寶發",
"桑美","凱米","碧利斯","艾維尼","珍珠","龍王","丹瑞","卡努"))
cloud <- cutter[data]
#---------------------------------------------------------------------------------------------看關聯性
tokens=list(cloud)
class(tokens)
# 建構詞彙
it = itoken(tokens, progressbar = FALSE)
vocab = create_vocabulary(it)
vocab = prune_vocabulary(vocab, term_count_min = 5L)#刪除出現小於5次的詞
tail(vocab,20)#查看最高频的詞
vectorizer = vocab_vectorizer(vocab)
tcm = create_tcm(it, vectorizer,skip_grams_window = 5L)# 考慮詞的前後5個詞
glove = GlobalVectors$new(word_vectors_size = 40, vocabulary = vocab, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01)
dim(wv_main)
wv_context = glove$components
dim(wv_context)
wv_main[1,1]
t(wv_context)[1,1]
word_vectors = wv_main + t(wv_context)
#建造“垃圾+廢物”向量
relation = word_vectors["廢物", , drop = FALSE] +
word_vectors["垃圾", , drop = FALSE]
#看和 “垃圾+廢物” 相關的10個詞
cos_sim = sim2(x = word_vectors, y = relation, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
# ----------------------------------------------------------------------------------------文字雲
z <- sort(table(cloud),decreasing = T)[1:50]
wordcloud2(z,
size = 2,
color = "random-light",
backgroundColor = "black")
# ----------------------------------------------------------------------------------------文字雲
z <- sort(table(cloud),decreasing = T)[1:60]
wordcloud2(z,
size = 2,
color = "random-light",
backgroundColor = "black")
wordcloud2(z,
size = 3,
color = "random-light",
backgroundColor = "black")
save(file = "ppt.RDATA")
library(rvest)
library(tm)
library(jiebaR)
library(jiebaRD)
library(Matrix)
library(RCurl)
library(wordcloud2)
library(text2vec)
library(stringr)
# ----------------------------------------------------------------------------------------抓網址
curl <- getCurlHandle()
curlSetOpt(cookie="over18=1", followlocation = TRUE, curl=curl)
links <- character()
front <- "https://www.ptt.cc/bbs/Gossiping/search?page="
for (i in 1:36) {
page <- paste0(front, i)
pages <- paste0(page, "&q=%E9%A2%B1%E9%A2%A8")
pages <- getURL(pages, curl=curl)
web <- read_html(pages) %>%
html_nodes(".title a") %>%
html_attr("href")
web <- paste0("https://www.ptt.cc", web)
print(web)
links <- c(links, web)
}
# -------------------------------------------------------------------------------------------抓資料
data <- c()
data1 <- c()
n = length(links)
for (i in 1: n) {
print(i)
lk <- getURL(links[i], curl = curl)
data1 <- read_html(lk)%>%
html_nodes("#main-content")%>%
html_text()
data <-paste0(data, data1)
}
# ----------------------------------------------------------------------------------------開始清洗
data <- as.list(data)    ##轉成list
d.corpus <- Corpus(VectorSource(data)) %>% # Corpus(VectorSource())的input是list
tm_map(removePunctuation) %>%  ## 標點符號
tm_map(removeNumbers) %>%     ##數字
tm_map(function(word) { # Regular Expression 把英文&數字的內容拿掉
gsub("[A-Za-z0-9]", "", word)    ##把 function(word) 裡面的"[A-Za-z0-9]" 換成 ""
})
for (i in 2:56)#------------------------------------------------------------------------看d.corpus的elements有幾個
{
d.corpus <- paste0(d.corpus[1], d.corpus[i])#全部文章連起來
}
# ----------------------------------------------------------------------------------------開始斷詞
data <- as.character(d.corpus)
cutter <- worker()
cutter <- worker(stop_word = "C:/Users/jeff6/Desktop/Github/alan/颱風專題/ptt/a.txt")
new_user_word(cutter, c("西半部","東半部","中南部","南部","北部","東部","西部","東北部","基隆",
"台北","新北","桃園","新竹","苗栗","台中","澎湖","金門","連江","馬祖",
"彰化","南投","雲林","嘉義","台南","高雄","屏東","台東","花蓮","宜蘭",
"外圍環流","能不能","明天",
"丹娜絲","山竹","瑪莉亞","泰利","谷超","天鴿","珊瑚","馬莎",
"海棠","尼莎","艾利","梅姬","馬勒卡","莫蘭蒂","尼伯特","杜鵑","天鵝",
"蘇迪勒","蓮花","昌鴻","紅霞","鳳凰","麥德姆","哈吉貝","菲特","天兔",
"康芮","潭美","西馬隆","蘇力","杰拉華","天秤","啟德","海葵","蘇拉",
"杜蘇芮","南瑪都","梅花","米雷","桑達","凡那比","南修","萊羅克",
"芭瑪","莫拉克","莫拉菲","薔蜜","哈格比","辛樂克","如麗","鳳凰",
"卡玫基","米塔","柯羅莎","韋帕","聖帕","梧提","帕布","珊珊","寶發",
"桑美","凱米","碧利斯","艾維尼","珍珠","龍王","丹瑞","卡努"))
cloud <- cutter[data]
#---------------------------------------------------------------------------------------------看關聯性
tokens=list(cloud)
class(tokens)
# 建構詞彙
it = itoken(tokens, progressbar = FALSE)
vocab = create_vocabulary(it)
vocab = prune_vocabulary(vocab, term_count_min = 5L)#刪除出現小於5次的詞
tail(vocab,20)#查看最高频的詞
vectorizer = vocab_vectorizer(vocab)
tcm = create_tcm(it, vectorizer,skip_grams_window = 5L)# 考慮詞的前後5個詞
glove = GlobalVectors$new(word_vectors_size = 40, vocabulary = vocab, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01)
dim(wv_main)
wv_context = glove$components
dim(wv_context)
wv_main[1,1]
t(wv_context)[1,1]
word_vectors = wv_main + t(wv_context)
#建造“垃圾+廢物”向量
relation = word_vectors["廢物", , drop = FALSE] +
word_vectors["垃圾", , drop = FALSE]
#看和 “垃圾+廢物” 相關的10個詞
cos_sim = sim2(x = word_vectors, y = relation, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
# ----------------------------------------------------------------------------------------文字雲
z <- sort(table(cloud),decreasing = T)[1:60]
wordcloud2(z,
size = 3,
color = "random-light",
backgroundColor = "black")
library(rvest)
library(tm)
library(jiebaR)
library(jiebaRD)
library(Matrix)
library(RCurl)
library(wordcloud2)
library(text2vec)
library(stringr)
# ----------------------------------------------------------------------------------------抓網址
curl <- getCurlHandle()
curlSetOpt(cookie="over18=1", followlocation = TRUE, curl=curl)
links <- character()
front <- "https://www.ptt.cc/bbs/Gossiping/search?page="
for (i in 1:36) {
page <- paste0(front, i)
pages <- paste0(page, "&q=%E9%A2%B1%E9%A2%A8")
pages <- getURL(pages, curl=curl)
web <- read_html(pages) %>%
html_nodes(".title a") %>%
html_attr("href")
web <- paste0("https://www.ptt.cc", web)
print(web)
links <- c(links, web)
}
# -------------------------------------------------------------------------------------------抓資料
data <- c()
data1 <- c()
n = length(links)
for (i in 1: n) {
print(i)
lk <- getURL(links[i], curl = curl)
data1 <- read_html(lk)%>%
html_nodes("#main-content")%>%
html_text()
data <-paste0(data, data1)
}
# ----------------------------------------------------------------------------------------開始清洗
data <- as.list(data)    ##轉成list
d.corpus <- Corpus(VectorSource(data)) %>% # Corpus(VectorSource())的input是list
tm_map(removePunctuation) %>%  ## 標點符號
tm_map(removeNumbers) %>%     ##數字
tm_map(function(word) { # Regular Expression 把英文&數字的內容拿掉
gsub("[A-Za-z0-9]", "", word)    ##把 function(word) 裡面的"[A-Za-z0-9]" 換成 ""
})
for (i in 2:56)#------------------------------------------------------------------------看d.corpus的elements有幾個
{
d.corpus <- paste0(d.corpus[1], d.corpus[i])#全部文章連起來
}
# ----------------------------------------------------------------------------------------開始斷詞
data <- as.character(d.corpus)
cutter <- worker()
cutter <- worker(stop_word = "C:/Users/jeff6/Desktop/Github/alan/颱風專題/ptt/a.txt")
new_user_word(cutter, c("西半部","東半部","中南部","南部","北部","東部","西部","東北部","基隆",
"台北","新北","桃園","新竹","苗栗","台中","澎湖","金門","連江","馬祖",
"彰化","南投","雲林","嘉義","台南","高雄","屏東","台東","花蓮","宜蘭",
"外圍環流","能不能","明天",
"丹娜絲","山竹","瑪莉亞","泰利","谷超","天鴿","珊瑚","馬莎",
"海棠","尼莎","艾利","梅姬","馬勒卡","莫蘭蒂","尼伯特","杜鵑","天鵝",
"蘇迪勒","蓮花","昌鴻","紅霞","鳳凰","麥德姆","哈吉貝","菲特","天兔",
"康芮","潭美","西馬隆","蘇力","杰拉華","天秤","啟德","海葵","蘇拉",
"杜蘇芮","南瑪都","梅花","米雷","桑達","凡那比","南修","萊羅克",
"芭瑪","莫拉克","莫拉菲","薔蜜","哈格比","辛樂克","如麗","鳳凰",
"卡玫基","米塔","柯羅莎","韋帕","聖帕","梧提","帕布","珊珊","寶發",
"桑美","凱米","碧利斯","艾維尼","珍珠","龍王","丹瑞","卡努"))
cloud <- cutter[data]
#---------------------------------------------------------------------------------------------看關聯性
tokens=list(cloud)
class(tokens)
# 建構詞彙
it = itoken(tokens, progressbar = FALSE)
vocab = create_vocabulary(it)
vocab = prune_vocabulary(vocab, term_count_min = 5L)#刪除出現小於5次的詞
tail(vocab,20)#查看最高频的詞
vectorizer = vocab_vectorizer(vocab)
tcm = create_tcm(it, vectorizer,skip_grams_window = 5L)# 考慮詞的前後5個詞
glove = GlobalVectors$new(word_vectors_size = 40, vocabulary = vocab, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01)
dim(wv_main)
wv_context = glove$components
dim(wv_context)
wv_main[1,1]
t(wv_context)[1,1]
word_vectors = wv_main + t(wv_context)
#建造“垃圾+廢物”向量
relation = word_vectors["廢物", , drop = FALSE] +
word_vectors["垃圾", , drop = FALSE]
#看和 “垃圾+廢物” 相關的10個詞
cos_sim = sim2(x = word_vectors, y = relation, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
# ----------------------------------------------------------------------------------------文字雲
z <- sort(table(cloud),decreasing = T)[1:60]
wordcloud2(z,
size = 3,
color = "random-light",
backgroundColor = "black")
for (i in 1:36) {
page <- paste0(front, i)
pages <- paste0(page, "&q=%E9%A2%B1%E9%A2%A8")
pages <- getURL(pages, curl=curl)
web <- read_html(pages) %>%
html_nodes(".title a") %>%
html_attr("href")
web <- paste0("https://www.ptt.cc", web)
print(web)
links <- c(links, web)
}
library(rvest)
library(tm)
library(jiebaR)
library(jiebaRD)
library(Matrix)
library(RCurl)
library(wordcloud2)
library(text2vec)
library(stringr)
# ----------------------------------------------------------------------------------------抓網址
curl <- getCurlHandle()
curlSetOpt(cookie="over18=1", followlocation = TRUE, curl=curl)
links <- character()
front <- "https://www.ptt.cc/bbs/Gossiping/search?page="
for (i in 1:36) {
page <- paste0(front, i)
pages <- paste0(page, "&q=%E9%A2%B1%E9%A2%A8")
pages <- getURL(pages, curl=curl)
web <- read_html(pages) %>%
html_nodes(".title a") %>%
html_attr("href")
web <- paste0("https://www.ptt.cc", web)
print(web)
links <- c(links, web)
}
# -------------------------------------------------------------------------------------------抓資料
data <- c()
data1 <- c()
n = length(links)
for (i in 1: n) {
print(i)
lk <- getURL(links[i], curl = curl)
data1 <- read_html(lk)%>%
html_nodes("#main-content")%>%
html_text()
data <-paste0(data, data1)
}
# ----------------------------------------------------------------------------------------開始清洗
data <- as.list(data)    ##轉成list
d.corpus <- Corpus(VectorSource(data)) %>% # Corpus(VectorSource())的input是list
tm_map(removePunctuation) %>%  ## 標點符號
tm_map(removeNumbers) %>%     ##數字
tm_map(function(word) { # Regular Expression 把英文&數字的內容拿掉
gsub("[A-Za-z0-9]", "", word)    ##把 function(word) 裡面的"[A-Za-z0-9]" 換成 ""
})
for (i in 2:56)#------------------------------------------------------------------------看d.corpus的elements有幾個
{
d.corpus <- paste0(d.corpus[1], d.corpus[i])#全部文章連起來
}
# ----------------------------------------------------------------------------------------開始斷詞
data <- as.character(d.corpus)
cutter <- worker()
cutter <- worker(stop_word = "C:/Users/jeff6/Desktop/Github/alan/颱風專題/ptt/a.txt")
new_user_word(cutter, c("西半部","東半部","中南部","南部","北部","東部","西部","東北部","基隆",
"台北","新北","桃園","新竹","苗栗","台中","澎湖","金門","連江","馬祖",
"彰化","南投","雲林","嘉義","台南","高雄","屏東","台東","花蓮","宜蘭",
"外圍環流","能不能","明天",
"丹娜絲","山竹","瑪莉亞","泰利","谷超","天鴿","珊瑚","馬莎",
"海棠","尼莎","艾利","梅姬","馬勒卡","莫蘭蒂","尼伯特","杜鵑","天鵝",
"蘇迪勒","蓮花","昌鴻","紅霞","鳳凰","麥德姆","哈吉貝","菲特","天兔",
"康芮","潭美","西馬隆","蘇力","杰拉華","天秤","啟德","海葵","蘇拉",
"杜蘇芮","南瑪都","梅花","米雷","桑達","凡那比","南修","萊羅克",
"芭瑪","莫拉克","莫拉菲","薔蜜","哈格比","辛樂克","如麗","鳳凰",
"卡玫基","米塔","柯羅莎","韋帕","聖帕","梧提","帕布","珊珊","寶發",
"桑美","凱米","碧利斯","艾維尼","珍珠","龍王","丹瑞","卡努"))
cloud <- cutter[data]
#---------------------------------------------------------------------------------------------看關聯性
tokens=list(cloud)
class(tokens)
# 建構詞彙
it = itoken(tokens, progressbar = FALSE)
vocab = create_vocabulary(it)
vocab = prune_vocabulary(vocab, term_count_min = 5L)#刪除出現小於5次的詞
tail(vocab,20)#查看最高频的詞
vectorizer = vocab_vectorizer(vocab)
tcm = create_tcm(it, vectorizer,skip_grams_window = 5L)# 考慮詞的前後5個詞
glove = GlobalVectors$new(word_vectors_size = 40, vocabulary = vocab, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01)
dim(wv_main)
wv_context = glove$components
dim(wv_context)
wv_main[1,1]
t(wv_context)[1,1]
word_vectors = wv_main + t(wv_context)
#建造“垃圾+廢物”向量
relation = word_vectors["廢物", , drop = FALSE] +
word_vectors["垃圾", , drop = FALSE]
#看和 “垃圾+廢物” 相關的10個詞
cos_sim = sim2(x = word_vectors, y = relation, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
# ----------------------------------------------------------------------------------------文字雲
z <- sort(table(cloud),decreasing = T)[1:60]
wordcloud2(z,
size = 3,
color = "random-light",
backgroundColor = "black")
load("C:/Users/jeff6/Desktop/Github/alan/颱風專題/ptt/.RData")
library(rvest)
library(tm)
library(jiebaR)
library(jiebaRD)
library(Matrix)
library(RCurl)
library(wordcloud2)
library(text2vec)
library(stringr)
# ----------------------------------------------------------------------------------------抓網址
curl <- getCurlHandle()
curlSetOpt(cookie="over18=1", followlocation = TRUE, curl=curl)
links <- character()
front <- "https://www.ptt.cc/bbs/Gossiping/search?page="
for (i in 1:36) {
page <- paste0(front, i)
pages <- paste0(page, "&q=%E9%A2%B1%E9%A2%A8")
pages <- getURL(pages, curl=curl)
web <- read_html(pages) %>%
html_nodes(".title a") %>%
html_attr("href")
web <- paste0("https://www.ptt.cc", web)
print(web)
links <- c(links, web)
}
# -------------------------------------------------------------------------------------------抓資料
data <- c()
data1 <- c()
n = length(links)
for (i in 1: n) {
print(i)
lk <- getURL(links[i], curl = curl)
data1 <- read_html(lk)%>%
html_nodes("#main-content")%>%
html_text()
data <-paste0(data, data1)
}
# ----------------------------------------------------------------------------------------開始清洗
data <- as.list(data)    ##轉成list
d.corpus <- Corpus(VectorSource(data)) %>% # Corpus(VectorSource())的input是list
tm_map(removePunctuation) %>%  ## 標點符號
tm_map(removeNumbers) %>%     ##數字
tm_map(function(word) { # Regular Expression 把英文&數字的內容拿掉
gsub("[A-Za-z0-9]", "", word)    ##把 function(word) 裡面的"[A-Za-z0-9]" 換成 ""
})
data <- as.character(d.corpus)
cutter <- worker()
cutter <- worker(stop_word = "C:/Users/jeff6/Desktop/Github/alan/颱風專題/ptt/a.txt")
new_user_word(cutter, c("西半部","東半部","中南部","南部","北部","東部","西部","東北部","基隆",
"台北","新北","桃園","新竹","苗栗","台中","澎湖","金門","連江","馬祖",
"彰化","南投","雲林","嘉義","台南","高雄","屏東","台東","花蓮","宜蘭",
"外圍環流","能不能","明天",
"丹娜絲","山竹","瑪莉亞","泰利","谷超","天鴿","珊瑚","馬莎",
"海棠","尼莎","艾利","梅姬","馬勒卡","莫蘭蒂","尼伯特","杜鵑","天鵝",
"蘇迪勒","蓮花","昌鴻","紅霞","鳳凰","麥德姆","哈吉貝","菲特","天兔",
"康芮","潭美","西馬隆","蘇力","杰拉華","天秤","啟德","海葵","蘇拉",
"杜蘇芮","南瑪都","梅花","米雷","桑達","凡那比","南修","萊羅克",
"芭瑪","莫拉克","莫拉菲","薔蜜","哈格比","辛樂克","如麗","鳳凰",
"卡玫基","米塔","柯羅莎","韋帕","聖帕","梧提","帕布","珊珊","寶發",
"桑美","凱米","碧利斯","艾維尼","珍珠","龍王","丹瑞","卡努"))
cloud <- cutter[data]
#---------------------------------------------------------------------------------------------看關聯性
tokens=list(cloud)
class(tokens)
# 建構詞彙
it = itoken(tokens, progressbar = FALSE)
vocab = create_vocabulary(it)
vocab = prune_vocabulary(vocab, term_count_min = 5L)#刪除出現小於5次的詞
tail(vocab,20)#查看最高频的詞
vectorizer = vocab_vectorizer(vocab)
tcm = create_tcm(it, vectorizer,skip_grams_window = 5L)# 考慮詞的前後5個詞
glove = GlobalVectors$new(word_vectors_size = 40, vocabulary = vocab, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01)
dim(wv_main)
wv_context = glove$components
dim(wv_context)
wv_main[1,1]
t(wv_context)[1,1]
word_vectors = wv_main + t(wv_context)
#建造“垃圾+廢物”向量
relation = word_vectors["廢物", , drop = FALSE] +
word_vectors["垃圾", , drop = FALSE]
#看和 “垃圾+廢物” 相關的10個詞
cos_sim = sim2(x = word_vectors, y = relation, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
# ----------------------------------------------------------------------------------------文字雲
z <- sort(table(cloud),decreasing = T)[1:60]
wordcloud2(z,
size = 3,
color = "random-light",
backgroundColor = "black")
